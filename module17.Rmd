---
title: "module17"
author: "Emiley Garcia-Zych"
date: "`r Sys.Date()`"
output: html_document
---

# Module 17. Generalized Linear Models

```{r}
library(curl)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/graddata.csv")
d <- read.csv(f, header = TRUE, sep = ",")
head(d)
```

```{r}
summary(d)
```

```{r}
par(mfrow = c(1, 2))
plot(as.factor(d$admit), d$gpa, xlab = "Admit", ylab = "GPA", col = "lightgreen")
plot(as.factor(d$admit), d$gre, xlab = "Admit", ylab = "GRE", col = "lightblue")
```

```{r}
pairs(d)
```

```{r}
table(d$admit, d$rank)
```

```{r}
# glm of admit~gre
glm <- glm(data = d, admit ~ gre, family = "binomial")
summary(glm)
```

```{r}
x <- seq(from = min(d$gre), to = max(d$gre), length.out = 1000)
logOR <- predict(glm, newdata = data.frame(gre = x))  # this function will predict the log(odds ratio)... but if we add the argument type='response', the predict() function will return the expected response on the scale of the Y variable, i.e., Pr(Y)=1, rather than the odds ratio!
y <- predict(glm, newdata = data.frame(gre = x), type = "response")
plot(d$admit ~ d$gre, pch = 21, type = "p", xlab = "GRE Score", ylab = "Pr(Y)",
    main = "Pr(Y) versus GRE")
lines(y ~ x, type = "l")
```

```{r}
ORchange <- exp(glm$coefficients[2])
ORchange  # a 1 unit increase in gre results in a 0.36% increase in likelihood of admission
```

```{r}
library(broom)
glmresults <- tidy(glm)
wald <- glmresults$estimate[2]/glmresults$std.error[2]
p <- 2 * (1 - pnorm(wald))  # calculation of 2 tailed p value associated with the Wald statistic
p
```

```{r}
CI <- confint.default(glm, level = 0.95)  # this function returns CIs based on standard errors, the way we have calculated them by hand previously... note the slight difference
CI
```

```{r}
CI <- glmresults$estimate[2] + c(-1, 1) * qnorm(0.975) * glmresults$std.error[2]  # and this is how we have calculated CIs by hand previously
CI
```

```{r}
CI <- confint(glm, level = 0.95)  # this function returns a CI based on log-likelihood, an iterative ML process
CI
```

```{r}
library(tidyverse)
```

```{r}
# create a label for your variables:
dat <- data.frame(index = c(1), labels = c("gre"), OR = c(exp(glm$coefficients[2])),
    LL = c((exp(glm$coefficients[2] - CI[2, 1]))), UL = c((exp(glm$coefficients[2] +
        CI[2, 2]))))
dat
```

```{r}
plot1 <- ggplot(dat, aes(y = index, x = OR)) + geom_point(shape = 18, size = 5) +
    geom_errorbarh(aes(xmin = LL, xmax = UL), height = 0.25) + geom_vline(xintercept = 1,
    color = "red", linetype = "dashed", cex = 1, alpha = 0.5) + scale_y_continuous(name = "",
    breaks = 1, labels = dat$label, trans = "reverse") + xlab("Odds Ratio (95% CI)") +
    ylab(" ") + theme_bw() + theme(panel.border = element_blank(), panel.background = element_blank(),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
    axis.line = element_line(colour = "black"), axis.text.y = element_text(size = 12,
        colour = "black"), axis.text.x.bottom = element_text(size = 12, colour = "black"),
    axis.title.x = element_text(size = 12, colour = "black"))
plot1
```

```{r}
library(ggstats)
ggcoef_model(glm, exponentiate = TRUE)
```

## Challenge 1

```{r}
glm <- glm(data = d, admit ~ gpa, family = "binomial")
summary(glm)
coeffs <- glm$coefficients
coeffs
CI <- confint(glm, level = 0.95)
CI
ORchange <- exp(coeffs[2])
ORchange
ORchangeCI <- exp(CI[2, ])
ORchangeCI
```

```{r}
library(ggplot2)
x <- data.frame(gpa = seq(from = 2, to = 4, length.out = 100))
prediction <- cbind(gpa = x, response = predict(glm, newdata = x, type = "response"))
# IMPORTANT: Using type='response' returns predictions on the scale of our
# Y variable, in this case Pr(admit); using the default for type would
# return a prediction on the logit scale, i.e., the log(odds ratio), or
# log(Pr(admit)/(1-Pr(admit)))
head(prediction)
```

```{r}
p <- ggplot(prediction, aes(x = gpa, y = response)) + geom_line() + xlab("GPA") +
    ylab("Pr(admit)")
p
```

```{r}
prediction <- cbind(gpa = x, predict(glm, newdata = x, type = "response", se = TRUE))
prediction$LL <- prediction$fit - 1.96 * prediction$se.fit
prediction$UL <- prediction$fit + 1.96 * prediction$se.fit
head(prediction)
```

```{r}
p <- ggplot(prediction, aes(x = gpa, y = fit))
p <- p + geom_ribbon(aes(ymin = LL, ymax = UL), alpha = 0.2) + geom_line() +
    xlab("GPA") + ylab("Pr(admit)")
p <- p + geom_point(data = d, aes(x = gpa, y = admit))
p

```

```{r}
ggcoef_model(glm, exponentiate = TRUE)
```

```{r}
glm1 <- glm(data = d, admit ~ 1, family = "binomial")
glm2 <- glm(data = d, admit ~ gpa, family = "binomial")
anova(glm1, glm2, test = "Chisq")

```

```{r}
library(lmtest)
lrtest(glm1, glm2)
```

We can also perform a likelihood ratio test by hand by taking the difference between the deviances of the two models. The deviance for a generalized linear model is analogous to the the residual sum of squares for a general linear model (low deviance, low RSS = better model). It is calculated as a kind of "distance" of given model from a fully "saturated" model, i.e., a model where each data point has its own parameters. The likelihood of the saturated model = 1 so its log-likelihood is log(1) = 0.

Deviance = 2 × (log-likelihood of the saturated model - log-likelihood of the proposed model)

Deviance = 2 × (0 - log-likelihood of the proposed model)

Deviance = -2 × (log-likelihood of the proposed model)

We can get the deviance associated with a given model object by accessing its \$deviance slot or by using the deviance() function with the model object as an argument.

## Challenge 2

Using the same "graddata.csv" dataset, run a multiple logistic regression analysis using gpa, gre, and rank to look at student admissions to graduate school. Do not, at first, include interaction terms.

What variables are significant predictors of the log(odds ratio) of admission?

What is the value of the log(odds ratio) coefficient and the 95% CIs around that value for the two continuous variable (gpa and gre), when taking the effects of the other and of rank into account? What do these translate into on the actual odds ratio scale?

Is the model including all three predictors better than models that include just two predictors?

Compare a model that includes the three predictors with no interactions versus one that includes the three predictors and all possible interactions.

```{r}
d$rank <- as.factor(d$rank)  # make sure rank is a categorical variable
glmGGR <- glm(data = d, formula = admit ~ gpa + gre + rank, family = binomial)  # 3 predictor model
summary(glmGGR)
coeff <- glmGGR$coefficients  # extract coefficients... all significantly different from 0
coeffCI <- cbind(coeff, confint(glmGGR))  # and 95% CIs around them... none include 0
```

```{r}
coeffCI
ORcoeff <- exp(coeff)
ORcoeff
ORcoeffCI <- exp(coeffCI)
ORcoeffCI
# Compare 2 verus 3 factor models
glmGG <- glm(data = d, formula = admit ~ gpa + gre, family = binomial)
glmGR <- glm(data = d, formula = admit ~ gpa + rank, family = binomial)
glmRG <- glm(data = d, formula = admit ~ gre + rank, family = binomial)
anova(glmGG, glmGGR, test = "Chisq")

anova(glmGR, glmGGR, test = "Chisq")

anova(glmRG, glmGGR, test = "Chisq")

# Compare model with and model without interactions
glmNO <- glm(data = d, admit ~ rank + gpa + gre, family = "binomial")
glmALL <- glm(data = d, admit ~ rank * gpa * gre, family = "binomial")
anova(glmNO, glmALL, test = "Chisq")  # adding interaction terms to model doesn't significantly decrease deviance
```

```{r}
ggcoef_model(glmGGR, exponentiate = TRUE)
```
