---
title: "module14"
author: "Emiley Garcia-Zych"
date: "`r Sys.Date()`"
output: html_document
---

# Module 14. Basic Categorical Data Analysis and ANOVA

install packages {curl}, {ggplot2}, {dplyr}, {car}

## Categorical Predictors in Regression 

Thus far we have used simple linear regression models involving continuous explanatory variables, but we can also use a discrete or categorical explanatory variable, made up of 2 or more groups that are coded as \"factors\" (i.e., we use integer values from 1 to k discrete groups as dummy values for our categorical variables). Let\'s load in our zombie survivor data again, but this time we specify `stringsAsFactors = TRUE` and then look at the class of the variable \"gender\".

```{r}
library(curl)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/zombies.csv")
z <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
class(z$gender)
summary(z$gender)
plot(z$height ~ z$gender)
```

```{r}
m <- lm(data = z, height ~ gender)
summary(m)
```

```{r}
levels(z$gender)
```

The estimate for β1 is reported as \"genderMale\" and the value for that coefficient, 4.0349, is the estimated difference in mean height associated with being a male compared to a female. The regression equation is basically:

**height = 65.5888 + 4.0349 x gender**, with males assigned a gender value of 1 and females of 0.

In this case, the **p** value associated with the **t** statistic for β1 is extremely low, so we conclude that \"gender\" has a significant effect on height.

We can easily `relevel()` what is the baseline group (this becomes much more useful as we get more categorical variables in our regressions). The result is very similar, but the sign of β1 is changed.

```{r}
z$gender <- relevel(z$gender, ref = "Male")
m <- lm(data = z, height ~ gender)
summary(m)
```

```{r}
p <- 1 - pf(276.9, df1 = 1, df2 = 998)
p
z$occupation <- "temp"
unique(z$major)
```

```{r}
levels(z$major)
row(data.frame(levels(z$major)))
```

```{r}
z$occupation[row(data.frame(levels(z$major))) %in% c(1, 2, 3, 5, 6, 14, 15,
    16, 18, 21, 23)] <- "natural science"
z$occupation[row(data.frame(levels(z$major))) %in% c(7, 8, 12, 17, 19, 22)] <- "logistics"
z$occupation[row(data.frame(levels(z$major))) %in% c(4, 18, 20)] <- "engineering"
z$occupation[row(data.frame(levels(z$major))) %in% c(9, 10, 11, 13, 24, 25,
    26)] <- "other"
z$occupation <- as.factor(z$occupation)
levels(z$occupation)
```

```{r}
z$occupation <- relevel(z$occupation, ref = "natural science")
levels(z$occupation)
plot(data = z, zombies_killed ~ occupation)
```

```{r}
m <- lm(data = z, zombies_killed ~ occupation)
summary(m)
```

```{r}
p <- 1 - pf(0.526, df1 = 3, df2 = 996)  # F test
p
```

## One Way ANOVA

Regression with a single categorical predictor run as we have just done above is exactly equivalent to a \"one-way\" or \"one-factor\" analysis of variance, or ANOVA. That is, ANOVA is just one type of special case of least squares regression.

We can thus run an ANOVA with one line in ***R***. Compare the results presented in the `summary()`output table from an ANOVA with that from the global test reported in `summary()` from `lm()`

```{r}
m <- aov(data = z, zombies_killed ~ occupation)
summary(m)
par(mfrow = c(2, 2))
```

## Challenge 1

```{r}
library(curl)
library(dplyr)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/gibbon-femurs.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
d$age <- factor(d$age, levels = c("inf", "juv", "subadult", "adult"))  #this reorders the age levels so that they're in order
head(d)
```

```{r}
hist(d$femur.length)
```

```{r}
qqnorm(d$femur.length)
```

```{r}
plot(data = d, femur.length ~ age)  # boxplot with medians
means <- summarise(group_by(d, age), mean(femur.length))  # calculate average by group
points(1:4, means$`mean(femur.length)`, pch = 4, cex = 1.5)  # add means to plot
```

```{r}
sds <- summarise(group_by(d, age), sd(femur.length))
max(sds$`sd(femur.length)`)/min(sds$`sd(femur.length)`)  # check that variances are roughly equal (ratio of max/min is <2)
```

```{r}
means.centered <- d$femur.length - means[as.numeric(d$age), 2]  # subtract relevant group mean from each data point
qqnorm(means.centered$`mean(femur.length)`)  # graphical tests for normality
```

```{r}
hist(d$femur.length[d$age == "subadult"], main = "subadult")
qqnorm(d$femur.length[d$age == "subadult"])
hist(d$femur.length[d$age == "adult"], main = "adult")
qqnorm(d$femur.length[d$age == "adult"])
```

```{r}
par(mfrow = c(1, 1))
plot(data = d, femur.length ~ age)
```

```{r}
m <- aov(data = d, femur.length ~ age)  # femur length related to age
summary(m)
m <- lm(data = d, femur.length ~ age)
summary(m)
```

## Post-Hoc Tests and the Non-Parametric Kruskal-Wallis Test

After finding a significant omnibus F statistic in an ANOVA, we can test, post-hoc, what group means are different from one another using pairwise t tests with appropriate p value correction.

```{r}
pairwise.t.test(d$femur.length, d$age, p.adj = "bonferroni")

```

```{r}
m <- aov(d$femur.length ~ d$age)
posthoc <- TukeyHSD(m, "d$age", conf.level = 0.95)
posthoc  # all age-sex classes differ
```

The Kruskal-Wallis test is a nonparametric alternative to one-way ANOVA that relaxes the need for normality in the distribution of data in each group (the different groups should still have roughly equal variances, though). Essentially, rather than testing the null hypothesis that the means for each group do not differ we are instead testing the null hypothesis that the **medians** do not differ. The test converts the continuous response variable to a set of RANKS (i.e., it does a uniform transformation) and then works with those ranks. The **p** value associated with the K-W test statistic is evaluated against a Chi-Square distribution.

```{r}
m <- kruskal.test(data = d, femur.length ~ age)
m
d <- arrange(d, femur.length)  # use {dplyr} to sort by femur.length
d <- mutate(d, femur.rank = row(data.frame(d$femur.length)))  # use {dplyr} to add new variable of rank femur.length
m <- kruskal.test(data = d, femur.rank ~ age)
m
```

## Multiple Factor ANOVA

Sometimes the data we are interested in is characterized by multiple grouping variables (e.g., age and sex). In the case of the gibbon femur length data, we are interested in the **main effect** of each factor on the variable of interest (e.g., do femur lengths vary by age or sex) while accounting for the effects of the other factor. We may also be interested in any **interactive effects** among factors. Thus, in multiple factor ANOVA we are interested in testing several null hypotheses simultaneously: [1] that each factor has no effect on the mean of our continuous reponse variable and [2] that there are no interactive effects of sets of factors on the mean of our continuous reponse variable.

Model description and testing for multiple ANOVA is a simple extension of the formula notation which we\'ve used for single factors. First, though, let\'s quickly check that our groups have similar variance.

```{r}
library(ggplot2)
means <- summarise(group_by(d, age, sex), mean(femur.length))  # first we calculate averages by combination of factors
```

```{r}
means
```

```{r}
sds <- summarise(group_by(d, age, sex), sd(femur.length))  # first we calculate averages by combination of factors
sds
max(sds$`sd(femur.length)`)/min(sds$`sd(femur.length)`)  # check that variances in each group are roughly equal (ratio of max/min is <2)
p <- ggplot(data = d, aes(y = femur.length, x = sex)) + geom_boxplot() + facet_wrap(~age,
    ncol = 4)  # and let's plot what the data look like
# p <- p + geom_point() # uncommenting this shows all points
p <- p + stat_summary(fun.y = mean, colour = "darkgreen", geom = "point", shape = 8,
    size = 6)
p
```

```{r}
summary(aov(data = d, femur.length ~ age))
summary(aov(data = d, femur.length ~ sex))
m <- summary(aov(data = d, femur.length ~ age + sex))
m
m <- aov(data = d, femur.length ~ age + sex + age:sex)  # : operator includes specific interaction terms
summary(m)
m <- aov(data = d, femur.length ~ age * sex)  # * operator includes all interaction terms
summary(m)
```

```{r}
m <- lm(data = d, femur.length ~ age * sex)  # or using the lm() function...
summary(m)
interaction.plot(x.factor = d$age, xlab = "Age", trace.factor = d$sex, trace.label = "Sex",
    response = d$femur.length, fun = mean, ylab = "Mean Femuur Length")
```

```{r}
m1 <- aov(data = d, femur.length ~ age * sex)
summary(m1)
m2 <- aov(data = d, femur.length ~ sex * age)
summary(m2)
m1 <- lm(data = d, femur.length ~ age * sex)
summary(m1)
m2 <- lm(data = d, femur.length ~ sex * age)
summary(m2)
```

## Challenge 2

```{r}
obs.table <- table(z$occupation)  # returns the same as summary()
obs.table
exp.table <- rep(0.25 * length(z$occupation), 4)
exp.table
occupation.matrix <- data.frame(cbind(obs.table, exp.table, (obs.table - exp.table)^2/exp.table))
names(occupation.matrix) <- c("Oi", "Ei", "(Oi-Ei)^2/Ei")
occupation.matrix
X2 <- sum(occupation.matrix[, 3])
X2
p <- 1 - pchisq(X2, length(obs.table) - 1)
p
chisq.test(x = obs.table, p = c(0.25, 0.25, 0.25, 0.25))  # here p is a vector of expected proportions... default is uniform
chisq.test(x = obs.table)
chisq.test(x = obs.table, p = c(0.38, 0.12, 0.23, 0.27))  # with a different set of expected proportions... fail to reject H0

```

```{r}
obs.table = table(z$gender, z$occupation)
obs.table
mosaicplot(t(obs.table), main = "Contingency Table", col = c("darkseagreen",
    "gray"))  # t function transposes the table
```

```{r}
r <- rowSums(obs.table)  # row margins
r
c <- colSums(obs.table)  # column margins
c
nr <- nrow(obs.table)  # row dimensions
nr
nc <- ncol(obs.table)  # column dimensions
nc
exp.table <- matrix(rep(c, each = nr) * r/sum(obs.table), nrow = nr, ncol = nc,
    dimnames = dimnames(obs.table))  # calculates the product of c*r and divides by total
exp.table
X2 <- sum((obs.table - exp.table)^2/exp.table)
X2
p <- 1 - pchisq(X2, df = (nr - 1) * (nc - 1))
p
chisq.test(x = obs.table)
```
